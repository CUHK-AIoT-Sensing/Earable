{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "# from torch.nn import CTCBeamSearchDecoder\n",
    "from torch.autograd import Variable\n",
    "# from ctcdecode import CTCBeamDecoder\n",
    "import multiprocessing as mp\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from random import randint, sample\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import h5py\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_data_based_on_labels(data, priority=\"test\"):\n",
    "    ### Split the data based on the labels\n",
    "    ### priority: \"train\" or \"test\"\n",
    "    ### train means n-1 duplicates in the training set, 1 in the testing set\n",
    "    \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    seen_labels = set()\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        # label = data[i, 0][0]\n",
    "        label = tuple(data[i, 0][0])\n",
    "        # prioritize the training data\n",
    "        if label not in seen_labels:\n",
    "            train_indices.append(i) if priority == \"test\" else test_indices.append(i)\n",
    "            seen_labels.add(label)\n",
    "        else:\n",
    "            test_indices.append(i) if priority == \"test\" else train_indices.append(i)\n",
    "    print(f\"Train indices: {train_indices}, Test indices: {test_indices}\")\n",
    "\n",
    "    return data[train_indices, :], data[test_indices, :]\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define a function to process the data\n",
    "def process_data(data, max_lab_length, max_seq_length, trainortest=\"test\", need_pad=True, label_class = 26, augment_times=20):\n",
    "    processed_data = []\n",
    "    lexicon = set()\n",
    "    unique_labels = set()\n",
    "    for i in range(data.shape[0]):\n",
    "        label = data[i, 0][0]\n",
    "        # skip if the label is not numeric\n",
    "        if any(isinstance(x, str) for x in label):\n",
    "            continue\n",
    "        # check if the element of label is all between 1 and 26\n",
    "        if not all(1 <= x <= label_class for x in label):\n",
    "            label = [x for x in label if 1 <= x <= label_class]\n",
    "            print(f\"Label {i} is not all between 1 and {label_class}, the label is: {data[i, 1]}, now changed to: {label}\")\n",
    "\n",
    "        sequence = data[i, 2]\n",
    "        # skip if the sequence is empty\n",
    "        if sequence.shape[0] == 0:\n",
    "            continue\n",
    "        original_label_length = len(label)\n",
    "        original_length = sequence.shape[0]\n",
    "        if original_length > max_seq_length:\n",
    "            print(f\"Sequence {i} is longer than max length: {original_length}\")\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "        # Pad the sequence and label\n",
    "        if need_pad:\n",
    "            if len(sequence.shape) > 2:\n",
    "                sequence = np.pad(sequence, ((0, max_seq_length - original_length), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                sequence = np.pad(sequence, ((0, max_seq_length - original_length), (0, 0)), 'constant', constant_values=0) \n",
    "            label = np.pad(label, (0, max_lab_length - len(label)), 'constant', constant_values=0)\n",
    "\n",
    "        sequence = sequence.astype(np.float32)  # Convert sequence to float32\n",
    "\n",
    "        # If training data, add augmented sequences\n",
    "        if trainortest == \"train\":\n",
    "            for j in range(3, max(data.shape[1], augment_times)):\n",
    "                if j >= data.shape[1]:\n",
    "                    augmented_sequence = sequence\n",
    "                else:\n",
    "                    augmented_sequence = data[i, j]\n",
    "                    if augmented_sequence.shape[0] == 0:\n",
    "                        # continue\n",
    "                        augmented_sequence = sequence\n",
    "                if need_pad:\n",
    "                    if len(augmented_sequence.shape) > 2:\n",
    "                        augmented_sequence = np.pad(augmented_sequence, ((0, max_seq_length - augmented_sequence.shape[0]), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "                    else:\n",
    "                        augmented_sequence = np.pad(augmented_sequence, ((0, max_seq_length - augmented_sequence.shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "                augmented_sequence = augmented_sequence.astype(np.float32)\n",
    "                processed_data.append((label, augmented_sequence, original_label_length, augmented_sequence.shape[0]))\n",
    "        \n",
    "        processed_data.append((label, sequence, original_label_length, original_length))\n",
    "\n",
    "        # Add words to lexicon\n",
    "        word = data[i, 1][0]\n",
    "        word = re.sub('[^a-zA-Z]', '', word).lower()\n",
    "        lexicon.add(word)\n",
    "\n",
    "        # Add labels to unique_labels\n",
    "        unique_labels.add(tuple(label))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # Write to lexicon.txt if testing data\n",
    "    if trainortest == \"test\":\n",
    "        with open('./lexicon.txt', 'w') as f:\n",
    "            for word in sorted(lexicon):\n",
    "                f.write(f\"{word} {' '.join(list(word))} |\\n\")\n",
    "        print(f\"Lexicon is written, its size is: {len(lexicon)}\")\n",
    "\n",
    "    return processed_data, unique_labels\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "        #                                    stride=stride, padding=padding, dilation=dilation))\n",
    "        # change the weight norm to layer norm\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                    stride=stride, padding=padding, dilation=dilation)\n",
    "        self.layernorm1 = nn.LayerNorm(n_outputs, eps = 1e-4)\n",
    "                                    \n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                        #    stride=stride, padding=padding, dilation=dilation))\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation)\n",
    "        self.layernorm2 = nn.LayerNorm(n_outputs, eps = 1e-4)\n",
    "\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.epsilon = 1e-5\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res + self.epsilon)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Modify the TCN model\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2, lstm_hidden=64):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1]*100, output_size)\n",
    "        \n",
    "        # self.lstm = nn.LSTM(num_channels[-1]*200, lstm_hidden, batch_first=True,num_layers = 2)\n",
    "        # # self.lstm = nn.GRU(num_channels[-1]*200, lstm_hidden, batch_first=True,num_layers = 2)\n",
    "        # self.linear = nn.Linear(lstm_hidden, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Inputs is a batch of sequences of dimension (batch_size, input_size, seq_len)\"\"\"\n",
    "        # Transpose the tensor to (batch_size, seq_len, input_size)\n",
    "        # inputs = inputs.transpose(1, 2)\n",
    "\n",
    "        # Reshape input to collapse the batch size and sequence length dimensions\n",
    "        # batch_size, seq_length, input_size = inputs.shape\n",
    "        if len(inputs.shape) == 3:\n",
    "            batch_size, seq_length, input_size = inputs.shape\n",
    "            inputs = inputs.view(batch_size * seq_length, 1, -1)\n",
    "        elif len(inputs.shape) == 4:\n",
    "            batch_size, seq_length, input_size, input_chl = inputs.shape\n",
    "            # permute the input to (batch_size * seq_length, input_chl, input_size)\n",
    "            inputs = inputs.permute(0, 1, 3, 2).contiguous().view(batch_size * seq_length, input_chl, -1)\n",
    "\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        \n",
    "        y = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        unfolded_y = y.view(batch_size, seq_length, -1)\n",
    "        # # flatten the output of TCN layer to (batch_size * seq_length, channel_size * features)\n",
    "        o = self.linear(unfolded_y) #y.view(batch_size * seq_length, -1)\n",
    "        # # print(o.shape)\n",
    "\n",
    "        # lstm_out, _ = self.lstm(unfolded_y)\n",
    "        # # lstm_out = lstm_out[:, -1, :]  # Taking the last output\n",
    "        # # print(lstm_out.shape)\n",
    "        # o = self.linear(lstm_out) # output of the linear layer\n",
    "        # # o = torch.stack([self.linear(lstm_out[i]) for i in range(lstm_out.shape[0])])\n",
    "\n",
    "        \n",
    "\n",
    "        return o,y\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, test_loader, num_epochs, optimizer, ctc_loss, scheduler=None, test_lexicon=None, output_size=27):\n",
    "    # define the labels of the dataset of the characters 0-26\n",
    "    # labels = np.arange(0, 27)\n",
    "    # define the labels of the alphabet list\n",
    "    labelchar = np.array(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y', 'z','|'])\n",
    "    # convert the labels to string\n",
    "    labelchar = [str(c) for c in labelchar]\n",
    "    # acquires the cpu number\n",
    "    num_cpu = mp.cpu_count()\n",
    "    # define the ctcdecoder\n",
    "    eps = 1e-6\n",
    "    # ctc_decoder = CTCBeamDecoder(labels, beam_width=100, log_probs_input=True, num_processes=round(num_cpu*0.8))\n",
    "    decoder = ctc_decoder(lexicon=\"./lexicon.txt\",tokens = labelchar, beam_size_token=500, blank_token=labelchar[0], sil_token=labelchar[27])\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(False)\n",
    "    \n",
    "    if torch.cuda.is_available() and test_lexicon is not None:\n",
    "        test_lexicon = torch.IntTensor(list(test_lexicon)).to(device)\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0\n",
    "        for i, (labels, sequences, original_label_lengths,original_lengths) in enumerate(train_loader):\n",
    "            # print shape of sequences\n",
    "\n",
    "            # Transfer data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                labels = labels.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                original_lengths = original_lengths.to(device)\n",
    "                original_label_lengths = original_label_lengths.to(device)\n",
    "                \n",
    "                \n",
    "\n",
    "            # randomly scale every element of the input sequence between (0.95, 1.05) to avoid overfitting\n",
    "            sequences = sequences * (0.95 + 0.1 * torch.rand(sequences.shape)).to(device)\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            batch_size, seq_length = sequences.shape[:2] # input_size[2], input_chl if len(sequences.shape) == 4\n",
    "            outputs,tcnout = model(sequences)          \n",
    "            if outputs.shape[2] != output_size:     \n",
    "                outputs = outputs.view(batch_size, seq_length, outputs.shape[1])\n",
    "            outputs = outputs.transpose(0, 1)  # (seq_length, batch_size, output_size)\n",
    "            \n",
    "            loss = ctc_loss(outputs.log_softmax(2).clamp(min=-10, max=10), labels, original_lengths, original_label_lengths) #\n",
    "            \n",
    "            \n",
    "            # stop training if the loss is less than 0.1 or nan\n",
    "            if math.isnan(loss.item()):\n",
    "                print(loss.item(), \"_\", epoch, \"_\", i)\n",
    "                loss = eps\n",
    "                # set all nan parameters of model to a random number\n",
    "                # for param in model.parameters():\n",
    "                #     torch.manual_seed(i)\n",
    "                #     param.data = torch.rand_like(param.data)*0.1-5e-2\n",
    "                #     # param.data = torch.where(torch.isnan(param.data), torch.rand_like(param.data)*0.1-5e-2, param.data)\n",
    "                # outputs,tcnout = model(sequences)          \n",
    "                # if outputs.shape[2] != 27:     \n",
    "                #     outputs = outputs.view(batch_size, seq_length, outputs.shape[1])\n",
    "                # outputs = outputs.transpose(0, 1)  # (seq_length, batch_size, output_size)\n",
    "                # loss = ctc_loss(outputs.log_softmax(2).clamp(min=-10, max=10), labels, original_lengths, original_label_lengths)\n",
    "        \n",
    "                print(labels)\n",
    "                # return model\n",
    "                # break\n",
    "                \n",
    "            loss_total += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            # clip the gradient to avoid exploding gradient\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        # if it is a reduceonplateau scheduler, then step the scheduler\n",
    "        if scheduler is not None and epoch > 0:\n",
    "            if scheduler.__class__.__name__ == \"ReduceLROnPlateau\":\n",
    "                scheduler.step(loss_total/len(train_loader))\n",
    "                print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        # Print the loss for this batch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_total/len(train_loader)}\")\n",
    "        if loss_total/len(train_loader) < 0.1:\n",
    "            print(\"original break due to loss\")\n",
    "            # break\n",
    "\n",
    "##################################################################\n",
    "#####----------- for every 10 epochs, evaluate the model----------\n",
    "##################################################################\n",
    "\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            # Evaluation on the test set\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "\n",
    "                rank_of_all = []\n",
    "                for ii, (labels, sequences,original_label_lengths, original_lengths) in enumerate(test_loader):\n",
    "                    \n",
    "                    # Transfer data to GPU if available\n",
    "                    if torch.cuda.is_available():\n",
    "                        labels = labels.to(device)\n",
    "                        sequences = sequences.to(device)\n",
    "                        original_lengths = original_lengths.to(device)\n",
    "                        original_label_lengths = original_label_lengths.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    batch_size, seq_length = sequences.shape[:2]\n",
    "                    outputs,_ = model(sequences)\n",
    "                    if len(outputs.shape) == 2:          \n",
    "                        outputs = outputs.view(batch_size, seq_length, outputs.shape[1])\n",
    "\n",
    "                    outputs = outputs.transpose(0, 1)  # to the shape of (seq_length, batch_size, output_size) #.clamp(min=-10, max=10)\n",
    "                    outputs_log_softmax = outputs.log_softmax(2).clamp(min=-10, max=10) # log softmax\n",
    "\n",
    "                    # Intuitive Decoder\n",
    "                    outputs_inte = []\n",
    "                    for i in range(len(original_lengths)):\n",
    "                        # cut the matrix to the original length\n",
    "                        outputs_i = outputs[:original_lengths[i],i, :].transpose(0,1)\n",
    "                        # return the greatest probability of each character\n",
    "                        outputs_i = torch.argmax(outputs_i, dim=0)\n",
    "\n",
    "                        # if there are consecutive same characters, only keep one\n",
    "                        for j in range(len(outputs_i)-1):\n",
    "                            if outputs_i[j] == outputs_i[j+1]:\n",
    "                                outputs_i[j+1] = 0\n",
    "                        # remove the 0s\n",
    "                        outputs_i = outputs_i[outputs_i.nonzero()]\n",
    "                        # convert the tensor to list\n",
    "                        outputs_inte.append(outputs_i.tolist())\n",
    "                    # print(outputs_inte)    \n",
    "\n",
    "\n",
    "                    # CTC decoder to get the final output and calculate the accuracy\n",
    "                    # The CTC decoder returns a list of lists\n",
    "                    # The first list contains the decoded outputs\n",
    "                    # The second list contains the output probabilities\n",
    "                    \n",
    "                    # decoded_outputs = decoder(outputs.transpose(0, 1).softmax(2).cpu(), original_lengths.cpu())\n",
    "                    # decoded_outputs = decoded_outputs[0][0].words\n",
    "                    # print(decoded_outputs[0])\n",
    "                    # print(decoded_outputs[0])\n",
    "                    # return decoded_outputs\n",
    "\n",
    "                    # treaverse the testlexicon with CTC loss of each sample, calculate the accuracy and the rank of the correct word\n",
    "                    if test_lexicon is not None:\n",
    "\n",
    "                        rank_of_batch = []\n",
    "                        for i in range(batch_size):\n",
    "                            loss_search = []\n",
    "                            label_i = labels[i,:original_label_lengths[i]]\n",
    "                            true_label_index = None\n",
    "                            outputs_i = outputs_log_softmax[:,i,:]\n",
    "                            for j, word in enumerate(test_lexicon):\n",
    "                                # Cut the word before the first zero (padding)\n",
    "                                word = word[:word.tolist().index(0) if 0 in word.tolist() else len(word)]\n",
    "\n",
    "                                # Check if the word is equal to the label\n",
    "                                if torch.equal(word, label_i):\n",
    "                                    if true_label_index is None:\n",
    "                                        true_label_index = j\n",
    "                                    else:\n",
    "                                        print(\"Repeat label in the lexicon!\")\n",
    "                                        continue\n",
    "\n",
    "                                # calculate the CTC loss of each word\n",
    "                                # try:\n",
    "                                torch.backends.cudnn.enabled = False if sequences.shape[1] > 1 else True\n",
    "                                loss_s = ctc_loss(outputs_i, word, original_lengths[i], torch.tensor([len(word)]).to(device)).item()\n",
    "                                torch.backends.cudnn.enabled = True\n",
    "                                # except:\n",
    "                                #     # check if outputs_i is cuda or cpu\n",
    "                                #     print(outputs_i.device)\n",
    "                                #     print(word.device)\n",
    "                                #     print(original_lengths[i].device)\n",
    "                                #     print(torch.tensor([len(word)]).to(device).device)\n",
    "                                if math.isnan(loss_s):\n",
    "                                    loss_s = 1e5\n",
    "                                loss_search.append(loss_s)\n",
    "                            # make sure the true label is in the lexicon\n",
    "                            if true_label_index is None:\n",
    "                               # raise ValueError(\"True label not in the lexicon!\")\n",
    "                               print(f\"Test label {label_i} not in the lexicon!\")\n",
    "                               continue\n",
    "                            \n",
    "                            # Get the rank of the true label\n",
    "                            sorted_loss = np.argsort(loss_search)\n",
    "                            rank_of_true_label = list(sorted_loss).index(true_label_index)\n",
    "                            rank_of_batch.append(rank_of_true_label)\n",
    "                        rank_of_all.append(rank_of_batch)\n",
    "                         \n",
    "\n",
    "                    # Calculate the CTC loss\n",
    "                    output_lengths = torch.full((batch_size,), outputs.shape[1], dtype=torch.long)\n",
    "                    try:\n",
    "                        loss = ctc_loss(outputs_log_softmax, labels, original_lengths, original_label_lengths)\n",
    "                    except:\n",
    "                        print(\"error\")\n",
    "                        print(outputs.shape)\n",
    "                        print(original_lengths)\n",
    "                        print(labels.shape)\n",
    "                        print(original_label_lengths)\n",
    "                        \n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {total_loss/len(test_loader)}\")\n",
    "                # flatten the rank_of_all\n",
    "                rank_of_all = [item for sublist in rank_of_all for item in sublist]\n",
    "                print(np.mean(rank_of_all)) # print the average rank of the correct word\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    return model, rank_of_all\n",
    "\n",
    "\n",
    "def get_lomo_train_data(word_list, sen_list, file_name, exp_mode, pr=0, ftsession=3, scenario=\"none\"):\n",
    "    train_data_raw = []\n",
    "    \n",
    "    # For \"lomoword\" mode\n",
    "    if exp_mode.startswith(\"lomo\"):\n",
    "        for word in word_list:\n",
    "            if word != file_name:\n",
    "                wordmat = loadmat('../UTokyo data/utokyo_word_fea_left_' + word + '.mat')\n",
    "                worddata = wordmat[list(wordmat.keys())[-1]][0,:]\n",
    "                # switch the chosen element by the scenario\n",
    "                if scenario == \"none\":\n",
    "                    train_data = np.concatenate(worddata[:5], axis=0)\n",
    "                elif scenario == \"headmotion\":\n",
    "                    train_data = worddata[5]\n",
    "                elif scenario == \"noise\":\n",
    "                    train_data = worddata[4]\n",
    "                elif scenario == \"walk\":\n",
    "                    train_data = worddata[6]\n",
    "                train_data_raw.append(train_data)\n",
    "                \n",
    "    # For \"lomoall\" mode\n",
    "    if exp_mode.startswith(\"lomoall\"):\n",
    "        prefix = file_name[:4]\n",
    "        for sen in sen_list:\n",
    "            if not sen.startswith(prefix):\n",
    "                senmat = loadmat('../UTokyo data/utokyo_sen_fea_left_' + sen + '.mat')\n",
    "                sendata = senmat[list(senmat.keys())[-1]][0,:]\n",
    "                data = np.concatenate(sendata[:4], axis=0)\n",
    "                # # elaminiate the empty sequences\n",
    "                # data = [ele for ele in data if ele.shape[1] != 0]\n",
    "                # eleminate the sequences with length less than 10\n",
    "                # data = np.array([ele[ele[:,2].shape[1]!=0] for ele in data])\n",
    "                train_data_raw.append(data)\n",
    "    if exp_mode.startswith(\"lomo\"):\n",
    "        train_data_raw = np.concatenate(train_data_raw, axis=0)\n",
    "    \n",
    "    # Fine-tuning data generation if pr is not 0\n",
    "    finetune_raw = None\n",
    "    if pr != 0:\n",
    "        if exp_mode.endswith(\"word\"):\n",
    "            wordmat = loadmat('../UTokyo data/utokyo_word_fea_left_' + file_name + '_spl6.mat')\n",
    "            data = wordmat[list(wordmat.keys())[-1]][0,:]\n",
    "            if scenario == \"none\": # test = word4\n",
    "                ftindex = [0, 1, 2, 4, 5, 6]\n",
    "            elif scenario == \"headmotion\": # test = word6\n",
    "                ftindex = [3, 4, 0, 1, 2, 6]\n",
    "            elif scenario == \"walk\": # test = word7\n",
    "                ftindex = [3, 4, 5, 0, 1, 2]\n",
    "        elif exp_mode.endswith(\"sen\"):\n",
    "            senmat = loadmat('../UTokyo data/utokyo_sen_fea_left_' + file_name + '_spl6.mat')\n",
    "            data = senmat[list(senmat.keys())[-1]][0,:]\n",
    "            ftindex = [0]\n",
    "\n",
    "        if ftsession > len(ftindex):\n",
    "            print(f\"fintune session {ftsession} is too large, now set to {len(ftindex)}\")\n",
    "            ftsession = len(ftindex)          \n",
    "        ft_data_set = data[ftindex[:ftsession]]\n",
    "        np.random.seed(581)\n",
    "        ft_data_set = [ft_data_set[i][np.random.choice(ft_data_set[i].shape[0], int(ft_data_set[i].shape[0]*pr), replace=False)] for i in range(len(ft_data_set))]\n",
    "        finetune_raw = np.concatenate(ft_data_set, axis=0)\n",
    "\n",
    "    return train_data_raw, finetune_raw\n",
    "\n",
    "def ctcdecode_byloss(model, test_loader, test_lexicon=None, test_lex_size = None):\n",
    "    with open(\"../Oxford 1035 Export pure_ext.csv\", newline=\"\") as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "        word_list_ori = [row[0] for row in data]\n",
    "        random.seed(100)\n",
    "        word_list = sample(word_list_ori, len(word_list_ori))\n",
    "        word_list_num = [[ord(c) - 96 for c in re.sub('[^a-zA-Z]', '', word).lower()] for word in word_list]\n",
    "        # pad the word_list_num to the max length with 0\n",
    "        word_list_num = [word + [0]*(max_word_length-len(word)) for word in word_list_num]\n",
    "    with open(\"../Corpus/MS_phrases2.csv\", newline=\"\",) as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "        sentences_ori = [row[0] for row in data[1:]]\n",
    "        # split the sentences into words, remove the empty words, then convert the words to numbers\n",
    "        w_in_sentences = [[ord(c) - 96 for c in re.sub('[^a-zA-Z]', '', word).lower()] for sentence in sentences_ori for word in sentence.split()]\n",
    "        # pad the w_in_sentences to the max length with 0\n",
    "        w_in_sentences = [word + [0]*(max_word_length-len(word)) for word in w_in_sentences]\n",
    "    # word_list = word_list + w_in_sentences\n",
    "    # add word_list to the lexicon until the size of lexicon is equal to test_lex_size or the word_list is empty\n",
    "    if test_lex_size is not None:\n",
    "        while len(test_lexicon) < test_lex_size and len(word_list_num) > 0:\n",
    "            try:\n",
    "                word = word_list_num.pop()\n",
    "            except:\n",
    "                print(\"word_list is empty!\")\n",
    "                break\n",
    "            test_lexicon.add(tuple(word))\n",
    "    print(f\"Lexicon size is: {len(test_lexicon)}\")\n",
    "            \n",
    "    test_lexicon = torch.IntTensor(list(test_lexicon))\n",
    "        \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        rank_of_all = []\n",
    "        for i, (labels, sequences,original_label_lengths, original_lengths) in enumerate(test_loader):\n",
    "            \n",
    "            # Transfer data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                labels = labels.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                original_lengths = original_lengths.to(device)\n",
    "                original_label_lengths = original_label_lengths.to(device)\n",
    "                test_lexicon = test_lexicon.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            # batch_size, seq_length, input_size = sequences.shape\n",
    "            batch_size, seq_length = sequences.shape[:2]\n",
    "            outputs,_ = model(sequences)\n",
    "            if len(outputs.shape) == 2:          \n",
    "                outputs = outputs.view(batch_size, seq_length, outputs.shape[1])\n",
    "\n",
    "            outputs = outputs.transpose(0, 1)  # to the shape of (seq_length, batch_size, output_size) #.clamp(min=-10, max=10)\n",
    "            outputs_log_softmax = outputs.log_softmax(2).clamp(min=-10, max=10) # log softmax\n",
    "\n",
    "            # treaverse the testlexicon with CTC loss of each sample, calculate the accuracy and the rank of the correct word\n",
    "            if test_lexicon is not None:\n",
    "\n",
    "                rank_of_batch = []\n",
    "                for i in range(batch_size):\n",
    "                    loss_search = []\n",
    "                    label_i = labels[i,:original_label_lengths[i]]\n",
    "                    true_label_index = None\n",
    "                    outputs_i = outputs_log_softmax[:,i,:]\n",
    "                    for j, word in enumerate(test_lexicon):\n",
    "                        # Cut the word before the first zero (padding)\n",
    "                        word = word[:word.tolist().index(0) if 0 in word.tolist() else len(word)]\n",
    "\n",
    "                        # Check if the word is equal to the label\n",
    "                        if torch.equal(word, label_i):\n",
    "                            if true_label_index is None:\n",
    "                                true_label_index = j\n",
    "                            else:\n",
    "                                print(\"Repeat label in the lexicon!\")\n",
    "                                continue\n",
    "\n",
    "                        # calculate the CTC loss of each word\n",
    "                        torch.backends.cudnn.enabled = False if sequences.shape[1] > 1 else True\n",
    "                        loss_s = ctc_loss(outputs_i, word, original_lengths[i], torch.tensor([len(word)]).to(device)).item()\n",
    "                        torch.backends.cudnn.enabled = True\n",
    "                        if math.isnan(loss_s):\n",
    "                            loss_s = 1e5\n",
    "                        loss_search.append(loss_s)\n",
    "                    # make sure the true label is in the lexicon\n",
    "                    if true_label_index is None:\n",
    "                    # raise ValueError(\"True label not in the lexicon!\")\n",
    "                        print(f\"Test label {label_i} not in the lexicon!\")\n",
    "                        continue                    \n",
    "                    # Get the rank of the true label\n",
    "                    sorted_loss = np.argsort(loss_search)\n",
    "                    rank_of_true_label = list(sorted_loss).index(true_label_index)\n",
    "                    rank_of_batch.append(rank_of_true_label)\n",
    "                rank_of_all.append(rank_of_batch)\n",
    "    # print the statistics of every element in the rank_of_all\n",
    "    rank_of_all = [item for sublist in rank_of_all for item in sublist]\n",
    "    topNacc = []\n",
    "    for i in range(1, 10):\n",
    "        topNacc.append(np.sum(np.array(rank_of_all) <= i) / len(rank_of_all))\n",
    "    # print topNacc with 3 decimal places\n",
    "    print([round(i, 3) for i in topNacc])\n",
    "    return topNacc #rank_of_all\n",
    "                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Max word length is: 14\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "with open(\"../Oxford 1035 Export pure_ext.csv\", newline=\"\") as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    word_list_ori = [row[0] for row in data[1:]]\n",
    "    # find the longest word length\n",
    "    max_word_length = max([len(word) for word in word_list_ori])\n",
    "    print(f\"Max word length is: {max_word_length}\")\n",
    "with open(\"../Corpus/MS_phrases2.csv\", newline=\"\",) as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    sentences_ori = [row[0] for row in data[1:]]\n",
    "    # split the sentences into words, remove the empty words, then convert the words to numbers\n",
    "    w_in_sentences = [[ord(c) - 96 for c in re.sub('[^a-zA-Z]', '', word).lower()] for sentence in sentences_ori for word in sentence.split()]\n",
    "    print(max([len(word) for word in w_in_sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Oxford 1035 Export pure_ext.csv\", newline=\"\") as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "    # convert the words to visemes, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_visemes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# print_visemes(\"bat\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print_visemes(\"bought\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print_visemes(\"boat\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print_visemes(\"shy\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print_visemes(\"jive\")\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mprint_visemes\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# \"Convert a word to a list of phonemes using CMU Pronouncing Dictionary.\".split()\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_visemes' is not defined"
     ]
    }
   ],
   "source": [
    "# print_visemes(\"bat\")\n",
    "# print_visemes(\"bought\")\n",
    "# print_visemes(\"boat\")\n",
    "# print_visemes(\"bait\")\n",
    "# print_visemes(\"bet\")\n",
    "# print_visemes(\"but\")\n",
    "# print_visemes(\"beat\")\n",
    "# print_visemes(\"bit\")\n",
    "# print_visemes(\"hang\")\n",
    "# print_visemes(\"vision\")\n",
    "# print_visemes(\"shy\")\n",
    "# print_visemes(\"jive\")\n",
    "# print_visemes(\"i've\")\n",
    "# \"Convert a word to a list of phonemes using CMU Pronouncing Dictionary.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'e-book' in 'e-book' not found in CMUdict.\n",
      "['AH0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download the CMU dictionary if you haven't already\n",
    "# nltk.download('cmudict')\n",
    "cmudict = nltk.corpus.cmudict.dict()\n",
    "cmudict[\"behaviour\"] = cmudict[\"behavior\"]\n",
    "cmudict[\"colour\"] = cmudict[\"color\"]\n",
    "cmudict[\"favourite\"] = cmudict[\"favorite\"]\n",
    "cmudict[\"covid\"] = [\"K\", \"OW\", \"V\", \"IH\", \"D\"]\n",
    "cmudict[\"racketball\"] = cmudict[\"racquetball\"]\n",
    "cmudict[\"ebook\"] = cmudict[\"e\"] + cmudict[\"book\"]\n",
    "\n",
    "\n",
    "# Define a phoneme-to-viseme mapping (simplified version)\n",
    "# phon_to_vise = {\n",
    "#     'AA': '1', 'AE': '2', 'AH': '1', 'AO': '1', 'AW': '3', 'AY': '3',\n",
    "#     'B': '4', 'CH': '5', 'D': '6', 'DH': '6', 'EH': '2', 'ER': '1',\n",
    "#     'EY': '2', 'F': '7', 'G': '8', 'HH': '9', 'IH': '2', 'IY': '2',\n",
    "#     'JH': '5', 'K': '8', 'L': '10', 'M': '4', 'N': '11', 'NG': '11',\n",
    "#     'OW': '3', 'OY': '3', 'P': '4', 'R': '12', 'S': '13', 'SH': '5',\n",
    "#     'T': '6', 'TH': '13', 'UH': '3', 'UW': '3', 'V': '7', 'W': '9',\n",
    "#     'Y': '9', 'Z': '13', 'ZH': '5'\n",
    "# }\n",
    "gpt_vise = {\n",
    "    'AA': '1', 'AH': '1', 'AO': '1', 'ER': '1', \n",
    "    'AE': '2', 'EH': '2', 'EY': '2', 'IH': '2', 'IY': '2',\t\n",
    "    'AW': '3', 'AY': '3', 'OY': '3', 'UH': '3','UW': '3','OW': '3',\n",
    "    'B': '4', 'M': '4', 'P': '4',\n",
    "    'CH': '5', 'JH': '5', 'SH': '5', 'ZH': '5',\n",
    "    'D': '6', 'DH': '6', 'T': '6',\n",
    "    'F': '7', 'V': '7',\n",
    "    'G': '8', 'K': '8',\n",
    "    'HH': '9', 'W': '9', 'Y': '9',\n",
    "    'L': '10', \n",
    "    'N': '11', 'NG': '11',\n",
    "    'R': '12',\n",
    "    'S': '13', 'Z': '13', 'TH': '13'\n",
    "    }\n",
    "# Define a phoneme-to-viseme mapping (Lee's version) \n",
    "## {/A/ /aʊ/ /ai/ /ʌ/}{/e/ /ei/ /æ/} {/i/ /I/} {/O/ /OI/ /@U/} {/U/ /u/}\n",
    "lee_vise = { ####Lee\n",
    "    'AA': '1', 'AW': '1', 'AY': '1', 'AH': '1', \n",
    "    'AE': '2', 'EH': '2', 'EY': '2', \n",
    "    'IH': '3', 'IY': '3',\n",
    "    'AO': '4', 'OY': '4', 'OW': '4',\n",
    "    'UH': '5', 'UW': '5',\n",
    "    'B': '6', 'P': '6', 'M': '6',\n",
    "    'CH': '7', 'JH': '7', 'SH': '7', 'ZH': '7',\n",
    "    'D': '8', 'T': '8', 'S': '8', 'Z': '8', 'TH': '8', 'DH': '8',\n",
    "    'F': '9', 'V': '9',\n",
    "    'G': '10', 'K': '10', 'N': '10', 'NG': '10', 'L': '10', 'Y': '10', 'HH': '10', \n",
    "    'R': '11', 'W': '11',\n",
    "    'ER': '12'\n",
    "    }\n",
    "\n",
    "disney_wood_vise = { ####　Disney Vowel and Woodward Consonant\n",
    "    'B': '1', 'P': '1', 'M': '1',\n",
    "    'D': '2', 'T': '2', 'N': '2','L': '2','TH': '2','DH': '2','S': '2','Z': '2','CH': '2','JH': '2','SH': '2','ZH': '2','Y': '2','K': '2','G': '2','HH': '2','NG': '2',\n",
    "    'F': '3', 'V': '3',\n",
    "    'R': '4', 'W': '4',\n",
    "    'IH': '5', 'IY': '5',\n",
    "    'UH': '6', 'UW': '6',\n",
    "    'OW': '7',\n",
    "    'AO': '8', 'OY': '8', 'AW': '8',\n",
    "    'AA': '9', 'AE': '9','ER': '9',   \n",
    "    'AH': '10', 'EH': '10', 'EY': '10',\t'AY': '10'\n",
    "    }\n",
    "\n",
    "disney_wood_9 = { ####　Disney Vowel and Woodward Consonant\n",
    "    'B': '1', 'P': '1', 'M': '1',\n",
    "    'D': '2', 'T': '2', 'N': '2','L': '2','TH': '2','DH': '2','S': '2','Z': '2','CH': '2','JH': '2','SH': '2','ZH': '2','Y': '2','K': '2','G': '2','HH': '2','NG': '2',\n",
    "    'F': '3', 'V': '3',\n",
    "    'R': '4', 'W': '4',\n",
    "    'IH': '5', 'IY': '5',\n",
    "    'UH': '6', 'UW': '6',\n",
    "    'OW': '7',\n",
    "    'AO': '8', 'OY': '8', 'AW': '8',\n",
    "    'AA': '9', 'AE': '9','ER': '9',   \n",
    "    'AH': '9', 'EH': '9', 'EY': '9','AY': '9'\n",
    "    }\n",
    "\n",
    "phon_to_phon = {\n",
    "    'AA': '1', 'AE': '2', 'AH': '3', 'AO': '4', 'AW': '5', 'AY': '6',\n",
    "    'B': '7', 'CH': '8', 'D': '9', 'DH': '10', 'EH': '11', 'ER': '12',\n",
    "    'EY': '13', 'F': '14', 'G': '15', 'HH': '16', 'IH': '17', 'IY': '18',\n",
    "    'JH': '19', 'K': '20', 'L': '21', 'M': '22', 'N': '23', 'NG': '24',\n",
    "    'OW': '25', 'OY': '26', 'P': '27', 'R': '28', 'S': '29', 'SH': '30',\n",
    "    'T': '31', 'TH': '32', 'UH': '33', 'UW': '34', 'V': '35', 'W': '36',\n",
    "    'Y': '37', 'Z': '38', 'ZH': '39'\n",
    "}\n",
    "\n",
    "# phon_to_vise = phon_to_phon\n",
    "# phon_to_vise = gpt_vise\n",
    "# phon_to_vise = lee_vise\n",
    "phon_to_vise = disney_wood_vise\n",
    "\n",
    "def count_visemes(phone2visemeDict):\n",
    "    viseme_count = set()\n",
    "    for phoneme, viseme in phone2visemeDict.items():\n",
    "        viseme_count.add(viseme)\n",
    "    return len(viseme_count)\n",
    "\n",
    "def remove_stress(phoneme):\n",
    "    return ''.join([char for char in phoneme if not char.isdigit()])\n",
    "\n",
    "def word_to_phonemes(word):\n",
    "    \"\"\"Convert a word to a list of phonemes using CMU Pronouncing Dictionary.\"\"\"\n",
    "    word = word.lower()\n",
    "    if ' ' in word[:-1]:\n",
    "        wordlist = word.split(' ')\n",
    "    else:\n",
    "        wordlist = [word]\n",
    "    phon_seq = []\n",
    "    for word_ele in wordlist:\n",
    "        if word_ele in cmudict:\n",
    "            # Take the first pronunciation entry\n",
    "            phon_ele = cmudict[word_ele][0]\n",
    "            phon_seq.extend(phon_ele)\n",
    "        else:\n",
    "            print(f\"Word '{word_ele}' in '{word}' not found in CMUdict.\")\n",
    "            return []\n",
    "    return [remove_stress(phoneme) for phoneme in phon_seq]\n",
    "\n",
    "def phonemes_to_visemes(phoneme_sequence, viseme_dict = phon_to_vise, ifviseme = True):\n",
    "    \"\"\"Convert a list of phonemes to visemes using a predefined mapping.\"\"\"\n",
    "    if ifviseme:\n",
    "        cvt_dict = viseme_dict\n",
    "    else:\n",
    "        cvt_dict = phon_to_phon\n",
    "    for phoneme in phoneme_sequence:\n",
    "        if phoneme not in cvt_dict:\n",
    "            print(f\"Phoneme '{phoneme}' not found in the viseme dictionary.\")\n",
    "            return []\n",
    "    visemes = [cvt_dict[phoneme] for phoneme in phoneme_sequence]\n",
    "    # visemes = [phon_to_vise.get(phoneme, '0') for phoneme in phoneme_sequence]\n",
    "    return visemes\n",
    "\n",
    "def print_visemes(word, ifviseme = True):    \n",
    "    phoneme_sequence = word_to_phonemes(word)\n",
    "    if phoneme_sequence:\n",
    "        print(f\"Phonemes for '{word}': {phoneme_sequence}\")\n",
    "        if ifviseme:\n",
    "            viseme_sequence = phonemes_to_visemes(phoneme_sequence)\n",
    "            print(f\"Visemes for '{word}': {viseme_sequence}\")\n",
    "\n",
    "\n",
    "\n",
    "def phoneme_labelling(data_ori, ifviseme = True, viseme_dict = phon_to_vise):\n",
    "    ## can be used for both csv and readed mat file\n",
    "    iscsv = isinstance(data_ori[0][0], str)\n",
    "    data_copy = data_ori.copy()\n",
    "    if iscsv:\n",
    "        data_copy = [data_copy]\n",
    "    elif len(data_ori.shape) > 1: ## if the data is mat file but is session data\n",
    "        data_copy = [data_copy]\n",
    "    viseme_lexicon = {}\n",
    "    word_to_viseme = {}\n",
    "    conflict_list = set()\n",
    "    phoneme_lack_list = set()\n",
    "    for session in data_copy:\n",
    "        for i in range(len(session)):\n",
    "            textword = session[i][1][0].lower() if not iscsv else session[i][0].lower()\n",
    "            if textword in word_to_viseme:\n",
    "                continue\n",
    "            phoneme_sequence = word_to_phonemes(textword)\n",
    "            if phoneme_sequence:\n",
    "                viseme_sequence = phonemes_to_visemes(phoneme_sequence, viseme_dict = viseme_dict, ifviseme = ifviseme) \n",
    "                cvt_seq = [int(x) for x in viseme_sequence]\n",
    "                # build the word_to_viseme dictionary\n",
    "                if textword not in word_to_viseme:\n",
    "                    word_to_viseme[textword] = np.array([cvt_seq])\n",
    "                # if viseme_lexicon do not have a key of the viseme_sequence, then add the viseme_sequence as the key and the word as the value\n",
    "                if tuple(cvt_seq) not in viseme_lexicon:\n",
    "                    viseme_lexicon[tuple(cvt_seq)] = textword\n",
    "                else:\n",
    "                    if textword != viseme_lexicon[tuple(cvt_seq)]:\n",
    "                        if (textword, viseme_lexicon[tuple(cvt_seq)]) not in conflict_list:\n",
    "                            print(f\"Conflict: {textword} and {viseme_lexicon[tuple(cvt_seq)]}\")\n",
    "                            conflict_list.add((textword, viseme_lexicon[tuple(cvt_seq)]))\n",
    "            else:\n",
    "                print(f\"Error, Word {textword} not in the CMU dictionary!\")\n",
    "                phoneme_lack_list.add(textword)\n",
    "    if len(phoneme_lack_list) > 0:\n",
    "        return phoneme_lack_list\n",
    "    if len(conflict_list) > -1:  ## \n",
    "    # if len(conflict_list) == 0: ## if there is no conflict    \n",
    "        print(\"No conflict in the lexicon!\")\n",
    "        for session in data_copy:\n",
    "            for i in range(len(session)):\n",
    "                textword = session[i][1][0].lower() if not iscsv else session[i][0].lower()\n",
    "                if textword in word_to_viseme:\n",
    "                    # if the data is csv, save viseme in 2nd column, otherwise save in 1st column\n",
    "                    if iscsv:\n",
    "                        session[i].append(word_to_viseme.get(textword, [[0]]))\n",
    "                    else:\n",
    "                        session[i][0] = word_to_viseme.get(textword, [[0]])\n",
    "                else:\n",
    "                    print(f\"Error, Word {textword} not in the word_to_viseme dictionary!\")\n",
    "        return data_copy\n",
    "    else:\n",
    "        print(\"Conflict in the lexicon!\")\n",
    "        return None\n",
    "# data_viseme = phoneme_labelling(data)\n",
    "# csvname = \"Oxford 1035 Export pure_ext.csv\"\n",
    "# with open(\"../Oxford 1035 Export pure_ext.csv\", newline=\"\") as csvfile:\n",
    "#     csvdata = list(csv.reader(csvfile))\n",
    "#     csv_vise = phoneme_labelling(csvdata)\n",
    "#     # save the viseme data to a csv file\n",
    "#     with open(\"./Oxford 1035 Export pure_ext_vise.csv\", \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(csv_vise)\n",
    "# count_visemes(phon_to_vise)\n",
    "# # Example usage:\n",
    "\n",
    "print_visemes(\"e-book\")\n",
    "print(cmudict[\"a\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ['AH']\n",
      "b ['B', 'IY']\n",
      "c ['S', 'IY']\n",
      "d ['D', 'IY']\n",
      "e ['IY']\n",
      "f ['EH', 'F']\n",
      "g ['JH', 'IY']\n",
      "h ['EY', 'CH']\n",
      "i ['AY']\n",
      "j ['JH', 'EY']\n",
      "k ['K', 'EY']\n",
      "l ['EH', 'L']\n",
      "m ['EH', 'M']\n",
      "n ['EH', 'N']\n",
      "o ['OW']\n",
      "p ['P', 'IY']\n",
      "q ['K', 'Y', 'UW']\n",
      "r ['AA', 'R']\n",
      "s ['EH', 'S']\n",
      "t ['T', 'IY']\n",
      "u ['Y', 'UW']\n",
      "v ['V', 'IY']\n",
      "w ['D', 'AH', 'B', 'AH', 'L', 'Y', 'UW']\n",
      "x ['EH', 'K', 'S']\n",
      "y ['W', 'AY']\n",
      "z ['Z', 'IY']\n"
     ]
    }
   ],
   "source": [
    "####--- Generate viseme for spelled words\n",
    "# print phoneme of a==>z\n",
    "for i in range(97, 123):\n",
    "    print(chr(i), word_to_phonemes(chr(i)))\n",
    "wordsample = \"hello\"\n",
    "# connect the phoneme of all characters in wordsample\n",
    "listlist_phon = [word_to_phonemes(c) for c in wordsample]\n",
    "list_phon = [phoneme for list_phon in listlist_phon for phoneme in list_phon] # left for goes ahead\n",
    "# print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordmat = loadmat('../BC data/'+ word_file +'.mat')\n",
    "# mat = wordmat\n",
    "# data = mat[list(mat.keys())[-1]]\n",
    "# data = data[0,:]\n",
    "# session = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell for Training BC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase Mode\n",
      "Subject: dxf_right_gngram_phrase_downup_spl6_str3_lag100_var, pr: 1, session: 3, scenario: none\n",
      "File dxf_right_gngram_phrase_downup_spl6_str3_lag100_var loaded!\n",
      "Conflict: per cent of the and percent of the\n",
      "No conflict in the lexicon!\n",
      "Word to viseme done!\n",
      "Input size is: 4\n",
      "Model, optimizer and scheduler deleted!\n",
      "125\n",
      "38\n",
      "Lexicon is written, its size is: 100\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (400, 3)\n",
      "Train data size: 300, Test data size: 100\n",
      "First few items: [array([[20, 21, 13, 22, 31, 34, 11, 23, 18, 27, 12, 31, 17, 20, 37,  3,\n",
      "         21, 12, 14,  1, 23, 31]])\n",
      " array(['CLAIM TO ANY PARTICULAR FONT'], dtype='<U28')\n",
      " array([[[-9.28214199e+00,  5.25925525e-01, -1.91974801e-01,\n",
      "          -9.23803502e+00],\n",
      "         [-3.99525938e+01,  4.31299646e+00, -1.24345385e+00,\n",
      "          -3.97003462e+01],\n",
      "         [-1.05546539e+02,  1.63171821e+01, -3.23437637e+00,\n",
      "          -1.04964467e+02],\n",
      "         ...,\n",
      "         [-2.38270365e+00, -7.69486715e-01, -1.03815951e+00,\n",
      "          -1.79156669e+00],\n",
      "         [-5.41741731e-01, -1.52386772e-01, -2.47455747e-01,\n",
      "          -4.54167583e-01],\n",
      "         [-5.71090877e-02, -1.43003550e-02, -2.69720345e-02,\n",
      "          -5.21066725e-02]],\n",
      "\n",
      "        [[-9.24227092e+00,  4.70936769e-01, -1.42187327e-01,\n",
      "          -9.23240381e+00],\n",
      "         [-3.95803750e+01,  3.80995464e+00, -7.74217569e-01,\n",
      "          -3.96565937e+01],\n",
      "         [-1.03946953e+02,  1.42378354e+01, -1.22895797e+00,\n",
      "          -1.04754605e+02],\n",
      "         ...,\n",
      "         [-1.32240177e+00, -2.77185053e-01, -1.43112729e+00,\n",
      "          -1.88397539e+00],\n",
      "         [-2.74388668e-01, -3.26645633e-02, -3.14523396e-01,\n",
      "          -4.48584631e-01],\n",
      "         [-2.63467135e-02, -1.01861740e-03, -3.18646552e-02,\n",
      "          -4.85433425e-02]],\n",
      "\n",
      "        [[-9.23729716e+00,  4.56549709e-01, -1.46764191e-01,\n",
      "          -9.22190933e+00],\n",
      "         [-3.95381425e+01,  3.67052349e+00, -8.10451774e-01,\n",
      "          -3.96066107e+01],\n",
      "         [-1.03772995e+02,  1.36311071e+01, -1.36083923e+00,\n",
      "          -1.04751331e+02],\n",
      "         ...,\n",
      "         [-1.49283629e+00,  3.20983979e-01, -3.23025713e-01,\n",
      "          -3.02570671e+00],\n",
      "         [-2.82362997e-01,  9.09005093e-02, -9.78814741e-02,\n",
      "          -7.20771550e-01],\n",
      "         [-2.47044963e-02,  1.13451263e-02, -1.08279310e-02,\n",
      "          -7.88885867e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.05800927e+00,  3.96638606e-01, -9.27293266e-02,\n",
      "          -9.16010807e+00],\n",
      "         [-3.79982162e+01,  3.17209529e+00, -2.37093010e-01,\n",
      "          -3.90662623e+01],\n",
      "         [-9.79073015e+01,  1.19202548e+01,  1.34256460e+00,\n",
      "          -1.02604260e+02],\n",
      "         ...,\n",
      "         [-1.28923148e+00,  1.15638871e-01, -1.49014550e+00,\n",
      "          -2.80114642e+00],\n",
      "         [-2.61663946e-01,  6.43897027e-02, -2.48800968e-01,\n",
      "          -6.22695477e-01],\n",
      "         [-2.39221340e-02,  9.46358239e-03, -1.47438265e-02,\n",
      "          -6.48576066e-02]],\n",
      "\n",
      "        [[-9.17478666e+00,  4.78524523e-01, -1.46320391e-01,\n",
      "          -9.21423096e+00],\n",
      "         [-3.89672975e+01,  3.96082821e+00, -7.86317800e-01,\n",
      "          -3.94853898e+01],\n",
      "         [-1.01404704e+02,  1.52962892e+01, -1.23875797e+00,\n",
      "          -1.04003129e+02],\n",
      "         ...,\n",
      "         [-1.51753536e+00, -1.85759475e+00, -8.47241926e-01,\n",
      "          -2.29191909e+00],\n",
      "         [-2.99494967e-01, -3.70333813e-01, -1.38826365e-01,\n",
      "          -5.29220311e-01],\n",
      "         [-2.71797585e-02, -3.49218427e-02, -8.46714633e-03,\n",
      "          -5.65906785e-02]],\n",
      "\n",
      "        [[-9.23806334e+00,  4.63222852e-01, -1.29930455e-01,\n",
      "          -9.22443421e+00],\n",
      "         [-3.95165024e+01,  3.76363761e+00, -6.62964555e-01,\n",
      "          -3.95822075e+01],\n",
      "         [-1.03522759e+02,  1.41598717e+01, -7.57697225e-01,\n",
      "          -1.04391084e+02],\n",
      "         ...,\n",
      "         [-8.40848586e-01, -1.28705511e+00, -1.01156468e+00,\n",
      "          -2.54344499e+00],\n",
      "         [-1.66615340e-01, -2.76251199e-01, -1.98126000e-01,\n",
      "          -5.45827642e-01],\n",
      "         [-1.49903985e-02, -2.79294096e-02, -1.71405905e-02,\n",
      "          -5.36529740e-02]]])                               ]\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 1/30, Train Loss: 4.330016478785762\n",
      "Epoch 1/30, Test Loss: 7.809488296508789\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 2/30, Train Loss: 3.7913831361134847\n",
      "Epoch 2/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 3/30, Train Loss: 3.792835946966101\n",
      "Epoch 3/30, Test Loss: 7.809488296508789\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 4/30, Train Loss: 3.79222285411976\n",
      "Epoch 4/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 5/30, Train Loss: 3.790498286353217\n",
      "Epoch 5/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 6/30, Train Loss: 3.7887273509414108\n",
      "Epoch 6/30, Test Loss: 7.809488296508789\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 7/30, Train Loss: 3.789599839316474\n",
      "Epoch 7/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 8/30, Train Loss: 3.7882682454144514\n",
      "Epoch 8/30, Test Loss: 7.809488296508789\n",
      "49.52\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 9/30, Train Loss: 3.7501647143893773\n",
      "Epoch 9/30, Test Loss: 3.4259963035583496\n",
      "43.05\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 10/30, Train Loss: 3.37156939541852\n",
      "Epoch 10/30, Test Loss: 3.421121835708618\n",
      "41.19\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 11/30, Train Loss: 3.347784089335689\n",
      "Epoch 11/30, Test Loss: 3.3842077255249023\n",
      "36.38\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 12/30, Train Loss: 3.2964871480729845\n",
      "Epoch 12/30, Test Loss: 3.341081380844116\n",
      "27.63\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 13/30, Train Loss: 3.2279436680122657\n",
      "Epoch 13/30, Test Loss: 3.2734053134918213\n",
      "20.28\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 14/30, Train Loss: 3.140181508240876\n",
      "Epoch 14/30, Test Loss: 3.20809268951416\n",
      "15.36\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 15/30, Train Loss: 3.0753590078707096\n",
      "Epoch 15/30, Test Loss: 3.1872832775115967\n",
      "14.61\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 16/30, Train Loss: 3.05579457671554\n",
      "Epoch 16/30, Test Loss: 3.1759352684020996\n",
      "14.81\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 17/30, Train Loss: 3.0366349651195383\n",
      "Epoch 17/30, Test Loss: 3.1855199337005615\n",
      "13.7\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 18/30, Train Loss: 3.015945981343587\n",
      "Epoch 18/30, Test Loss: 3.170529365539551\n",
      "12.02\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 19/30, Train Loss: 2.9976658955326787\n",
      "Epoch 19/30, Test Loss: 3.155979633331299\n",
      "13.11\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 20/30, Train Loss: 2.9761789664515743\n",
      "Epoch 20/30, Test Loss: 3.15844464302063\n",
      "11.75\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 21/30, Train Loss: 2.958093673917982\n",
      "Epoch 21/30, Test Loss: 3.1583099365234375\n",
      "11.51\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 22/30, Train Loss: 2.9363789021527325\n",
      "Epoch 22/30, Test Loss: 3.156606912612915\n",
      "11.89\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 23/30, Train Loss: 2.9117325835757786\n",
      "Epoch 23/30, Test Loss: 3.1647050380706787\n",
      "11.35\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 24/30, Train Loss: 2.8901031130331534\n",
      "Epoch 24/30, Test Loss: 3.166343927383423\n",
      "11.46\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 25/30, Train Loss: 2.8653654324566875\n",
      "Epoch 25/30, Test Loss: 3.1786718368530273\n",
      "10.9\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 26/30, Train Loss: 2.840107070075141\n",
      "Epoch 26/30, Test Loss: 3.175126075744629\n",
      "11.01\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 27/30, Train Loss: 2.8156396127630163\n",
      "Epoch 27/30, Test Loss: 3.17799973487854\n",
      "9.65\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 28/30, Train Loss: 2.7859371662139893\n",
      "Epoch 28/30, Test Loss: 3.18204402923584\n",
      "11.13\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 29/30, Train Loss: 2.779681249194675\n",
      "Epoch 29/30, Test Loss: 3.205174446105957\n",
      "10.27\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 30/30, Train Loss: 2.7728908192669905\n",
      "Epoch 30/30, Test Loss: 3.1912286281585693\n",
      "10.76\n",
      "lex_size: 100\n",
      "Lexicon size is: 100\n",
      "[0.44, 0.46, 0.49, 0.51, 0.52, 0.56, 0.59, 0.61, 0.66]\n",
      "Subject: dxf_right_gngram_phrase_downup_spl6_str3_lag100_var, pr: 1, session: 3, scenario: none_mtrain\n",
      "File dxf_right_gngram_phrase_downup_spl6_str3_lag100_var loaded!\n",
      "Conflict: per cent of the and percent of the\n",
      "No conflict in the lexicon!\n",
      "Word to viseme done!\n",
      "Input size is: 4\n",
      "Model, optimizer and scheduler deleted!\n",
      "125\n",
      "38\n",
      "Lexicon is written, its size is: 100\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (500, 3)\n",
      "Train data size: 400, Test data size: 100\n",
      "First few items: [array([[20, 21, 13, 22, 31, 34, 11, 23, 18, 27, 12, 31, 17, 20, 37,  3,\n",
      "         21, 12, 14,  1, 23, 31]])\n",
      " array(['CLAIM TO ANY PARTICULAR FONT'], dtype='<U28')\n",
      " array([[[-9.28214199e+00,  5.25925525e-01, -1.91974801e-01,\n",
      "          -9.23803502e+00],\n",
      "         [-3.99525938e+01,  4.31299646e+00, -1.24345385e+00,\n",
      "          -3.97003462e+01],\n",
      "         [-1.05546539e+02,  1.63171821e+01, -3.23437637e+00,\n",
      "          -1.04964467e+02],\n",
      "         ...,\n",
      "         [-2.38270365e+00, -7.69486715e-01, -1.03815951e+00,\n",
      "          -1.79156669e+00],\n",
      "         [-5.41741731e-01, -1.52386772e-01, -2.47455747e-01,\n",
      "          -4.54167583e-01],\n",
      "         [-5.71090877e-02, -1.43003550e-02, -2.69720345e-02,\n",
      "          -5.21066725e-02]],\n",
      "\n",
      "        [[-9.24227092e+00,  4.70936769e-01, -1.42187327e-01,\n",
      "          -9.23240381e+00],\n",
      "         [-3.95803750e+01,  3.80995464e+00, -7.74217569e-01,\n",
      "          -3.96565937e+01],\n",
      "         [-1.03946953e+02,  1.42378354e+01, -1.22895797e+00,\n",
      "          -1.04754605e+02],\n",
      "         ...,\n",
      "         [-1.32240177e+00, -2.77185053e-01, -1.43112729e+00,\n",
      "          -1.88397539e+00],\n",
      "         [-2.74388668e-01, -3.26645633e-02, -3.14523396e-01,\n",
      "          -4.48584631e-01],\n",
      "         [-2.63467135e-02, -1.01861740e-03, -3.18646552e-02,\n",
      "          -4.85433425e-02]],\n",
      "\n",
      "        [[-9.23729716e+00,  4.56549709e-01, -1.46764191e-01,\n",
      "          -9.22190933e+00],\n",
      "         [-3.95381425e+01,  3.67052349e+00, -8.10451774e-01,\n",
      "          -3.96066107e+01],\n",
      "         [-1.03772995e+02,  1.36311071e+01, -1.36083923e+00,\n",
      "          -1.04751331e+02],\n",
      "         ...,\n",
      "         [-1.49283629e+00,  3.20983979e-01, -3.23025713e-01,\n",
      "          -3.02570671e+00],\n",
      "         [-2.82362997e-01,  9.09005093e-02, -9.78814741e-02,\n",
      "          -7.20771550e-01],\n",
      "         [-2.47044963e-02,  1.13451263e-02, -1.08279310e-02,\n",
      "          -7.88885867e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.05800927e+00,  3.96638606e-01, -9.27293266e-02,\n",
      "          -9.16010807e+00],\n",
      "         [-3.79982162e+01,  3.17209529e+00, -2.37093010e-01,\n",
      "          -3.90662623e+01],\n",
      "         [-9.79073015e+01,  1.19202548e+01,  1.34256460e+00,\n",
      "          -1.02604260e+02],\n",
      "         ...,\n",
      "         [-1.28923148e+00,  1.15638871e-01, -1.49014550e+00,\n",
      "          -2.80114642e+00],\n",
      "         [-2.61663946e-01,  6.43897027e-02, -2.48800968e-01,\n",
      "          -6.22695477e-01],\n",
      "         [-2.39221340e-02,  9.46358239e-03, -1.47438265e-02,\n",
      "          -6.48576066e-02]],\n",
      "\n",
      "        [[-9.17478666e+00,  4.78524523e-01, -1.46320391e-01,\n",
      "          -9.21423096e+00],\n",
      "         [-3.89672975e+01,  3.96082821e+00, -7.86317800e-01,\n",
      "          -3.94853898e+01],\n",
      "         [-1.01404704e+02,  1.52962892e+01, -1.23875797e+00,\n",
      "          -1.04003129e+02],\n",
      "         ...,\n",
      "         [-1.51753536e+00, -1.85759475e+00, -8.47241926e-01,\n",
      "          -2.29191909e+00],\n",
      "         [-2.99494967e-01, -3.70333813e-01, -1.38826365e-01,\n",
      "          -5.29220311e-01],\n",
      "         [-2.71797585e-02, -3.49218427e-02, -8.46714633e-03,\n",
      "          -5.65906785e-02]],\n",
      "\n",
      "        [[-9.23806334e+00,  4.63222852e-01, -1.29930455e-01,\n",
      "          -9.22443421e+00],\n",
      "         [-3.95165024e+01,  3.76363761e+00, -6.62964555e-01,\n",
      "          -3.95822075e+01],\n",
      "         [-1.03522759e+02,  1.41598717e+01, -7.57697225e-01,\n",
      "          -1.04391084e+02],\n",
      "         ...,\n",
      "         [-8.40848586e-01, -1.28705511e+00, -1.01156468e+00,\n",
      "          -2.54344499e+00],\n",
      "         [-1.66615340e-01, -2.76251199e-01, -1.98126000e-01,\n",
      "          -5.45827642e-01],\n",
      "         [-1.49903985e-02, -2.79294096e-02, -1.71405905e-02,\n",
      "          -5.36529740e-02]]])                               ]\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 1/30, Train Loss: 4.245605483055114\n",
      "Epoch 1/30, Test Loss: 7.809488296508789\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 2/30, Train Loss: 3.794021085633172\n",
      "Epoch 2/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 3/30, Train Loss: 3.792016348838806\n",
      "Epoch 3/30, Test Loss: 7.809488296508789\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 4/30, Train Loss: 3.79306015835868\n",
      "Epoch 4/30, Test Loss: 7.809487819671631\n",
      "49.51\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 5/30, Train Loss: 3.770293431017134\n",
      "Epoch 5/30, Test Loss: 3.5082051753997803\n",
      "46.16\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 6/30, Train Loss: 3.3780940503544277\n",
      "Epoch 6/30, Test Loss: 3.460768699645996\n",
      "43.67\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 7/30, Train Loss: 3.3694179275300766\n",
      "Epoch 7/30, Test Loss: 3.4237146377563477\n",
      "43.13\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 8/30, Train Loss: 3.3664682367112904\n",
      "Epoch 8/30, Test Loss: 3.4347593784332275\n",
      "42.46\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 9/30, Train Loss: 3.357575433784061\n",
      "Epoch 9/30, Test Loss: 3.384572982788086\n",
      "40.06\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 10/30, Train Loss: 3.333387560579512\n",
      "Epoch 10/30, Test Loss: 3.3596575260162354\n",
      "34.99\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 11/30, Train Loss: 3.2910824942588808\n",
      "Epoch 11/30, Test Loss: 3.331883430480957\n",
      "27.18\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 12/30, Train Loss: 3.2346994495391845\n",
      "Epoch 12/30, Test Loss: 3.257147789001465\n",
      "21.25\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 13/30, Train Loss: 3.1665939572122364\n",
      "Epoch 13/30, Test Loss: 3.2400591373443604\n",
      "18.46\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 14/30, Train Loss: 3.116872108247545\n",
      "Epoch 14/30, Test Loss: 3.2230961322784424\n",
      "18.23\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 15/30, Train Loss: 3.062680827246772\n",
      "Epoch 15/30, Test Loss: 3.229008197784424\n",
      "15.53\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 16/30, Train Loss: 3.0414083035786947\n",
      "Epoch 16/30, Test Loss: 3.2177951335906982\n",
      "15.94\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 17/30, Train Loss: 3.0230231473180984\n",
      "Epoch 17/30, Test Loss: 3.22635555267334\n",
      "13.44\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 18/30, Train Loss: 3.0029326166046992\n",
      "Epoch 18/30, Test Loss: 3.2330172061920166\n",
      "13.87\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 19/30, Train Loss: 2.9835646080970766\n",
      "Epoch 19/30, Test Loss: 3.2515087127685547\n",
      "13.2\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 20/30, Train Loss: 2.9611362912919788\n",
      "Epoch 20/30, Test Loss: 3.235780954360962\n",
      "14.41\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 21/30, Train Loss: 2.9389217111799453\n",
      "Epoch 21/30, Test Loss: 3.251746654510498\n",
      "14.28\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 22/30, Train Loss: 2.9164194758733113\n",
      "Epoch 22/30, Test Loss: 3.27067494392395\n",
      "13.61\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 23/30, Train Loss: 2.893192389011383\n",
      "Epoch 23/30, Test Loss: 3.3037426471710205\n",
      "13.0\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 24/30, Train Loss: 2.8689402559068466\n",
      "Epoch 24/30, Test Loss: 3.3207812309265137\n",
      "13.77\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 25/30, Train Loss: 2.844400568538242\n",
      "Epoch 25/30, Test Loss: 3.331224203109741\n",
      "14.15\n",
      "Adjusting learning rate of group 0 to 6.0000e-05.\n",
      "Epoch 26/30, Train Loss: 2.820479127036201\n",
      "Epoch 26/30, Test Loss: 3.3533880710601807\n",
      "14.62\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 27/30, Train Loss: 2.796181488831838\n",
      "Epoch 27/30, Test Loss: 3.3812742233276367\n",
      "14.98\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 28/30, Train Loss: 2.7640744807985094\n",
      "Epoch 28/30, Test Loss: 3.374068021774292\n",
      "13.97\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 29/30, Train Loss: 2.754670185777876\n",
      "Epoch 29/30, Test Loss: 3.3861820697784424\n",
      "13.54\n",
      "Adjusting learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 30/30, Train Loss: 2.746378939151764\n",
      "Epoch 30/30, Test Loss: 3.382236957550049\n",
      "14.0\n",
      "lex_size: 100\n",
      "Lexicon size is: 100\n",
      "[0.33, 0.42, 0.47, 0.5, 0.52, 0.52, 0.57, 0.57, 0.57]\n"
     ]
    }
   ],
   "source": [
    "phon_to_vise = phon_to_phon\n",
    "# phon_to_vise = gpt_vise\n",
    "# phon_to_vise = lee_vise\n",
    "# phon_to_vise = disney_wood_vise #disney_wood_9\n",
    "\n",
    "# Define the parameters\n",
    "num_channels = [16,32]  # number of channels\n",
    "kernel_size = 11  # kernel size\n",
    "# input_size = 1 | 2  # number of input channels\n",
    "batch_size = 8 # Define the batch size\n",
    "batch_size_test = 128\n",
    "output_size = 1 + count_visemes(phon_to_vise)  # number of output channels\n",
    "normal_train_epoch = 30\n",
    "\n",
    "# change the device to the second GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define the CTC Loss function\n",
    "ctc_loss = CTCLoss(blank=0, reduction='mean',zero_infinity=True)\n",
    "\n",
    "# file_list = ['']\n",
    "\n",
    "# def train_with_file_list(mode=\"word\"):\n",
    "\n",
    "mode=\"phrase\"\n",
    "print(f\"{mode} Mode\")\n",
    "word_list = ['_dmu1hzy_normal'] #dxf_44100_right_100phrase_down_spl6_str3 hlx_50word5times_down_spl6_str3\n",
    "# word_list = glob.glob(\"../BC data/xlq_44100_50word_5times_down_spl6_*.mat\")\n",
    "# word_list = [word_l.split(\"\\\\\")[-1].split(\".\")[0] for word_l in word_list]\n",
    "\n",
    "word_list_right = []\n",
    "sen_list = []\n",
    "file_list = word_list if mode == \"word\" or \"phrase\" else sen_list\n",
    "\n",
    "# choose a sublist of file to train\n",
    "# subindex = [4]\n",
    "# file_list = [file_list[i] for i in subindex]\n",
    "acc_list = []\n",
    "pr = 0\n",
    "self_session = 3\n",
    "losocv = [\"leave1\",\"leave2\",\"leave3\",\"none\"] # [\"leave1\",\"leave2\",\"leave3\",\"none\",\"noise\",\"headmotion\",\"walk\"]\n",
    "lex_size = [100] #[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "# lex_size = None\n",
    "for file_name in file_list:\n",
    "    tcnmodel1 = None\n",
    "    train_data_raw = None\n",
    "    for pr in [1]: #[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:#[0, 0.2, 0.4, 0.6, 0.8]: #\n",
    "        for self_session in [3]: #np.arange(5): #\n",
    "            for scenario in [\"none\",\"none_mtrain\"]: #[\"readWord\"]: #losocv: # \n",
    "                print(f\"Subject: {file_name}, pr: {pr}, session: {self_session}, scenario: {scenario}\")\n",
    "                # Load the Matlab file\n",
    "                # wordmat = loadmat('../UTokyo data/utokyo_word_fea_left_'+ file_name +'.mat')\n",
    "                for sen_file in sen_list:\n",
    "                    if sen_file.startswith(file_name[:4]):\n",
    "                        senmat = loadmat('../UTokyo data/utokyo_sen_fea_left_'+ sen_file +'_spl6.mat')\n",
    "                        break\n",
    "                for word_file in word_list:\n",
    "                    if word_file.startswith(file_name[:4]):\n",
    "                        wordmat = loadmat('../BC data/'+ word_file +'.mat')\n",
    "                        print(f\"File {word_file} loaded!\")\n",
    "                        break\n",
    "                # for word_file_right in word_list_right:\n",
    "                #     if word_file_right.startswith(file_name[:4]):\n",
    "                #         wordmat_right = loadmat('../UTokyo data/utokyo_word_fea_right_'+ word_file_right +'.mat')\n",
    "                #         break\n",
    "\n",
    "                if mode == \"word\" or mode == \"phrase\":\n",
    "                    mat = wordmat\n",
    "                    # data1 = senmat[list(senmat.keys())[-1]][0,:] # data1 is the senmat data\n",
    "                elif mode == \"sen\":\n",
    "                    mat = senmat\n",
    "                    data1 = wordmat[list(wordmat.keys())[-1]][0,:] # data1 is the wordmat data\n",
    "                    \n",
    "                # dataRword = wordmat_right[list(wordmat_right.keys())[-1]][0,:]\n",
    "                data = mat[list(mat.keys())[-1]]\n",
    "                data = data[0,:]\n",
    "                # if scenario == \"readWord\":\n",
    "\n",
    "                kfold = KFold(n_splits=5, shuffle=True, random_state=120)        \n",
    "\n",
    "                if mode == \"phrase\":\n",
    "                    data = phoneme_labelling(data, viseme_dict=phon_to_vise)\n",
    "                    print(\"Word to viseme done!\")\n",
    "                    \n",
    "                if mode == \"word\" or mode == \"phrase\":\n",
    "                                    \n",
    "                    if scenario == \"none\":\n",
    "                        test_data_raw = data[3]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2]]), axis=0)\n",
    "                    elif scenario == \"none_mtrain\":\n",
    "                        test_data_raw = data[3]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2,4]]), axis=0)\n",
    "                    elif scenario == \"none_wtrain\":\n",
    "                        test_data_raw = data[3]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2,6]]), axis=0)\n",
    "                    elif scenario == \"leave1\":\n",
    "                        test_data_raw = data[0]\n",
    "                        train_data_raw = np.concatenate((data[[1,2,3]]), axis=0)\n",
    "                    elif scenario == \"leave2\":\n",
    "                        test_data_raw = data[1]\n",
    "                        train_data_raw = np.concatenate((data[[0,2,3]]), axis=0)\n",
    "                    elif scenario == \"leave3\":\n",
    "                        test_data_raw = data[2]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,3]]), axis=0)\n",
    "                    elif scenario == \"music\":\n",
    "                        test_data_raw = data[5]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2,3]]), axis=0)\n",
    "                    elif scenario == \"noise\":\n",
    "                        test_data_raw = data[4]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2,3]]), axis=0)\n",
    "                    elif scenario == \"walk\":\n",
    "                        test_data_raw = data[6]\n",
    "                        train_data_raw = np.concatenate((data[[0,1,2,3]]), axis=0)\n",
    "                        \n",
    "                    elif scenario == \"onesession\":\n",
    "                        all_data_raw = np.concatenate((data[:7]), axis=0) if len(data) > 1 else data[0]\n",
    "                        # test_data_raw = data[6]\n",
    "                        # train_data_raw, test_data_raw = split_data_based_on_labels(all_data_raw, priority=\"train\")\n",
    "                        data1234, data5 = split_data_based_on_labels(all_data_raw, priority=\"train\")\n",
    "                        data123, data4 = split_data_based_on_labels(data1234, priority=\"train\")\n",
    "                        data12, data3 = split_data_based_on_labels(data123, priority=\"train\")\n",
    "                        data_1, data2 = split_data_based_on_labels(data12, priority=\"train\")\n",
    "                        train_data_raw = np.concatenate((data_1, data5, data4, data2), axis=0)\n",
    "                        test_data_raw = data3\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "\n",
    "                if len(test_data_raw[0][2].shape) == 2:\n",
    "                    input_size = 1\n",
    "                else:\n",
    "                    input_size = test_data_raw[0][2].shape[2] # if the input channel number is 2, then the shape would be >3d, and 1 for 2d\n",
    "                print(f\"Input size is: {input_size}\")\n",
    "                # delete the model if it already exists\n",
    "                if 'model' in locals() and 'optimizer' in locals() and 'scheduler' in locals():\n",
    "                    del model\n",
    "                    del optimizer\n",
    "                    del scheduler\n",
    "                    del train_loader\n",
    "                    del test_loader\n",
    "                    del ctc_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # make sure local variable of function is deleted                    \n",
    "                    print(\"Model, optimizer and scheduler deleted!\")\n",
    "                    ctc_loss = CTCLoss(blank=0, reduction='mean',zero_infinity=True)\n",
    "                \n",
    "                finetune_raw = None\n",
    "\n",
    "                # # fetch train_data_raw from lomo function\n",
    "                # if train_data_raw is None:\n",
    "                #     train_data_raw, finetune_raw = get_lomo_train_data(word_list, sen_list, file_name, exp_mode=\"lomoword\"+mode, pr=pr, ftsession=self_session, scenario=scenario)\n",
    "                # else:\n",
    "                #     _, finetune_raw = get_lomo_train_data(word_list, sen_list, file_name, exp_mode=\"finetune\"+mode, pr=pr, ftsession=self_session, scenario=scenario)      \n",
    "\n",
    "\n",
    "                # # split by vocabulary\n",
    "                # train_data_raw, test_data_raw = split_data_based_on_labels(data[6])\n",
    "\n",
    "                # train_data_raw = data[0]\n",
    "                # load the test data from single file\n",
    "                # test_mat = loadmat('./fyt_4_1.mat')\n",
    "                # test_data_raw = test_mat[list(test_mat.keys())[-1]]\n",
    "\n",
    "\n",
    "                if finetune_raw is None:\n",
    "                    data = np.concatenate((train_data_raw,test_data_raw), axis=0)\n",
    "                else:\n",
    "                    data = np.concatenate((train_data_raw,test_data_raw,finetune_raw), axis=0)\n",
    "\n",
    "                # Find the maximum sequence length in the data\n",
    "                # fetch from the third column to last column\n",
    "                max_seq_length = max([sample.shape[0] for sample in data[:, 2:].reshape(-1)])\n",
    "                max_lab_length = max([sample[0].shape[0] for sample in data[:, 0] if type(sample[0]) == np.ndarray]) # fetch from the first column\n",
    "                max_word_length = max_lab_length \n",
    "                # max_lab_length = max_word_length # 14 # fetch from the csv\n",
    "\n",
    "                print(max_seq_length)\n",
    "                print(max_lab_length)\n",
    "\n",
    "\n",
    "                # Process the data\n",
    "                processed_train_data,train_lexicon = process_data(train_data_raw, max_lab_length, max_seq_length ,trainortest=\"train\", label_class=output_size-1) # \n",
    "                processed_test_data,test_lexicon = process_data(test_data_raw, max_lab_length, max_seq_length, label_class=output_size-1) #need_pad=False when batch_size=1\n",
    "\n",
    "                # # Split the data into training and testing datasets\n",
    "                # train_data, test_data = train_test_split(processed_data, test_size=0.2, random_state=0)\n",
    "\n",
    "                # Create the training and testing datasets\n",
    "                train_dataset = SequenceDataset(processed_train_data)\n",
    "                test_dataset = SequenceDataset(processed_test_data)\n",
    "\n",
    "                # Define the data loaders\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "                # Check its type\n",
    "                print(f\"Type: {type(data)}\")\n",
    "\n",
    "                # Check its shape\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "\n",
    "                # print size of train and test data\n",
    "                print(f\"Train data size: {len(train_data_raw)}, Test data size: {len(test_data_raw)}\")\n",
    "\n",
    "                # Print the first few items\n",
    "                print(f\"First few items: {data[0, :3]}\")\n",
    "\n",
    "                # Create the model\n",
    "                torch.manual_seed(581)\n",
    "\n",
    "                model = TCN(input_size, output_size, num_channels, kernel_size)\n",
    "\n",
    "                # Define the optimizer\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=0.01)\n",
    "                # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.3, verbose=True)\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=13, gamma=0.3, verbose=True)\n",
    "                # warm up the model\n",
    "                \n",
    "                model.to(device)\n",
    "                torch.backends.cudnn.enabled = True\n",
    "                # # use all the available GPUs\n",
    "                # if torch.cuda.device_count() > 1:\n",
    "                #     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "                #     model = nn.DataParallel(model)\n",
    "\n",
    "            # Train the model\n",
    "                if finetune_raw is not None:\n",
    "                    if tcnmodel1 is not None:\n",
    "                        print(\"Model already trained, use the previous model to fine tune\")\n",
    "                    else:\n",
    "                        tcnmodel1, rank_of_all = train_model(model, train_loader, test_loader, 10, optimizer, ctc_loss, scheduler, test_lexicon=test_lexicon, output_size=output_size)\n",
    "                        \n",
    "                    # print the name and pr of the file with the \"fine tune started\"\n",
    "                    print(f\"Fine tune started: {file_name}, Portion is {pr}\")\n",
    "                    processed_finetune_data, _ = process_data(finetune_raw, max_lab_length, max_seq_length, trainortest=\"train\")\n",
    "                    finetune_dataset = SequenceDataset(processed_finetune_data)\n",
    "                    finetune_loader = DataLoader(finetune_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    # Fine-tune the model using finetune_loader\n",
    "                    optimizer = optim.Adam(tcnmodel1.parameters(), lr=0.0002, weight_decay=0.01)\n",
    "                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3, verbose=True)\n",
    "                    tcnmodel2 = tcnmodel1\n",
    "                    tcnmodelft,rank_of_all = train_model(tcnmodel2, finetune_loader, test_loader, 5, optimizer, ctc_loss, scheduler, test_lexicon=test_lexicon, output_size=output_size)\n",
    "                else:\n",
    "                    tcnmodel1, rank_of_all = train_model(model, train_loader, test_loader, normal_train_epoch, optimizer, ctc_loss, scheduler, test_lexicon=test_lexicon, output_size=output_size)\n",
    "\n",
    "\n",
    "            # test different lexicon size\n",
    "                if mode == \"word\" or mode == \"phrase\":\n",
    "                    if lex_size is None: # original lexicon\n",
    "                        # print the statistics of every element in the rank_of_all\n",
    "                        topNacc = []\n",
    "                        for i in range(1, 10):\n",
    "                            topNacc.append(np.sum(np.array(rank_of_all) <= i) / len(rank_of_all))\n",
    "                        # print topNacc with 3 decimal places\n",
    "                        print([round(i, 3) for i in topNacc])\n",
    "                        # append topNacc, file_name, pr to acc_list\n",
    "                        acc_list.append([round(i, 5) for i in topNacc] + [file_name, scenario,pr, self_session, len(test_lexicon)])\n",
    "                    else: # test different lexicon size\n",
    "                        if mode == \"phrase\":\n",
    "                            if lex_size[0] > len(test_lexicon):\n",
    "                                lex_size.insert(0, len(test_lexicon))\n",
    "                        for lex_size_i in lex_size:\n",
    "                            # test_lexicon_i = set(random.sample(test_lexicon, lex_size_i))\n",
    "                            print(f\"lex_size: {lex_size_i}\")\n",
    "                            topNacc = ctcdecode_byloss(tcnmodel1, test_loader, test_lexicon=test_lexicon, test_lex_size=lex_size_i)\n",
    "                            # append topNacc, file_name, pr to acc_list\n",
    "                            acc_list.append([round(i, 5) for i in topNacc] + [file_name, scenario, pr, self_session,  lex_size_i])\n",
    "                elif mode == \"sen\":\n",
    "                    topNacc = []\n",
    "                # WER per phrase\n",
    "                    # acc_phrase = []\n",
    "                    # for phrase in test_data_phrase:\n",
    "                    #     process_phrase, _ = process_data(phrase, max_lab_length, max_seq_length)\n",
    "                    #     phrase_dataset = SequenceDataset(process_phrase)\n",
    "                    #     phrase_loader = DataLoader(phrase_dataset, batch_size=len(phrase_dataset), shuffle=False)\n",
    "                    #     acc_tmp = ctcdecode_byloss(tcnmodel1, phrase_loader, test_lexicon=test_lexicon)\n",
    "                    #     acc_phrase.append(acc_tmp)\n",
    "                    # # calculate the average accuracy of the phrase\n",
    "                    # topNacc = np.mean(acc_phrase, axis=0)\n",
    "                    # # append topNacc, file_name, pr, lex_size to acc_list\n",
    "                    # acc_list.append([round(i, 5) for i in topNacc] + [file_name, pr, self_session, len(test_lexicon)])\n",
    "                # WER of all phrases\n",
    "                    for i in range(1, 10):\n",
    "                        topNacc.append(np.sum(np.array(rank_of_all) <= i) / len(rank_of_all))\n",
    "                    # print topNacc with 3 decimal places\n",
    "                    print([round(i, 3) for i in topNacc])\n",
    "                    # append topNacc, file_name, pr to acc_list\n",
    "                    acc_list.append([round(i, 5) for i in topNacc] + [file_name, scenario,pr, self_session, len(test_lexicon)])\n",
    "\n",
    "                        \n",
    "\n",
    "                    \n",
    "\n",
    "#     return acc_list\n",
    "\n",
    "# acc_list = train_with_file_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 'dxf_right_gngram_phrase_downup_spl6_str3_lag100_var', 'leave1', 1, 3, 100], [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 'dxf_right_gngram_phrase_downup_spl6_str3_lag100_var', 'music', 1, 3, 100], [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 'dxf_right_gngram_phrase_downup_spl6_str3_lag100_var', 'noise', 1, 3, 100], [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 'dxf_right_gngram_phrase_downup_spl6_str3_lag100_var', 'walk', 1, 3, 100], [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 'dxf_right_gngram_phrase_downup_spl6_str3_lag100_var', 'none', 1, 3, 100]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "print(acc_list)\n",
    "# save the acc_list to a csv file\n",
    "with open('../results/test.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['top1','top2','top3','top4','top5','top6','top7','top8','top9','file_name','scenario','pr','num_session','lex_size'])\n",
    "    writer.writerows(acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
